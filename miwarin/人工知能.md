#人工知能

*  [ほぼ日刊イトイ新聞 - がんばれ森川くんの遺伝子くん](http://www.1101.com/morikawa/index_AI.html)

##正攻法

1.  入力
1.  字句解析
1.  構文解析
1.  文脈解析
1.  意味解析
1.  感情分析
1.  文章生成
1.  返答

まともにやると認知心理学も応用するので手間がかかりすぎる。研究者たちがやっていることを 1 から追うのは無謀。巨人の肩に乗るべき。

#人工無脳

知的に振る舞う。

1.  入力
1.  文章生成
1.  返答

文章生成するときに工夫する。

1.  ショット
1.  シーン
1.  シナリオ

1 度の入力だけでなく、過去から入力を蓄積していき、分析し、現在どういう状態におかれているのかを分析し、未来がどうなりそうなのかを予測する。など

1.  仕事つまんね
1.  上司うぜえ
1.  同僚が仕事できない
1.  顧客がバカ
1.  疲れた

最後の「疲れた」だけを見ると、たんに体力的に疲れただけなのか何なのか分からない。

しかし、過去に遡ると、このひとは疲弊しており精神的に疲れていることが想像できる。

*  [人工無脳のアーキテクチャ](http://www.ycf.nanet.co.jp/%7Eskato/muno/5model/index.html)
*  [人工無脳レビュー](http://www.ycf.nanet.co.jp/%7Eskato/muno/material/review.html)
*  [ししゃもを偲ぶ - xe-kdoo (2005-04-18)](http://yowaken.dip.jp/tdiary/20050418.html#p02)
*  [ミクさんでSiri対抗アプリを作ってみた - ブログなんだよもん](http://koduki.hatenablog.com/entry/2012/07/08/232836)
*  [かぜきょうの部屋の隅のあたり。 Botに感情値をつける](http://kazekyou.blog61.fc2.com/blog-entry-355.html)
*  [人工無能　ロイディ --- SSB](http://rogiken.org/SSB/reudy.html)

#日本語処理

##辞書

*  [公開資源/日本語評価極性辞書 - 東北大学 乾・岡﨑研究室](http://www.cl.ecei.tohoku.ac.jp/index.php?%E5%85%AC%E9%96%8B%E8%B3%87%E6%BA%90%2F%E6%97%A5%E6%9C%AC%E8%AA%9E%E8%A9%95%E4%BE%A1%E6%A5%B5%E6%80%A7%E8%BE%9E%E6%9B%B8)
*  [単語感情極性対応表](http://www.lr.pi.titech.ac.jp/%7Etakamura/pndic_ja.html)

##コーパス

*  [国立国語研究所の言語コーパス整備計画KOTONOHA](http://www.ninjal.ac.jp/kotonoha/ex_6.html)
*  [言語資源・サービス一覧](https://alaginrc.nict.go.jp/resources/nictmastar/resource-info/2010-07-27-08-04-21.html)
*  [Google Japan Blog: 大規模日本語 n-gram データの公開](http://googlejapan.blogspot.jp/2007/11/n-gram.html)
*  [日本語ウェブコーパス 2010](http://s-yata.jp/corpus/nwc2010/)
*  [コーパス日本語学のための情報館 - コーパス紹介](http://www30.atwiki.jp/corpus-ling/pages/17.html)
*  [ウェブコーパスの作成方法と内容，現状での課題 - やた＠はてな日記](http://d.hatena.ne.jp/s-yata/20100707/1278524542)
*  [通知表の所見欄に書きたいけど書けない（性格を表す）言葉を書きかえる一覧表](http://www.kyo-sin.net/reframe.htm)
*  [名言集.com - 名言・ことわざ・金言・格言集めました](http://www.meigensyu.com/)
*  [座右の銘にしたい名言集](http://za-yu.com/)
*  [天元突破グレンラガンの名言](http://www.earth-words.net/human/gurenragan.html)


#機械学習

機械学習は、基本的には教師つき学習

    入力項目→(パラメーター)→評価関数

評価関数を育てることが「学習」である。

*  [教師あり学習 - Wikipedia](http://ja.wikipedia.org/wiki/%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92)
*  評価関数
       *  物事の良し悪しの判断基準を教えること
       *  人間が教えこんでいく
       *  判断基準の構造は決定木などがある
*  [平成22年度夏学期 数理情報工学特論第一 【機械学習とデータマイニング】](http://www.geocities.jp/kashi_pong/course_machine_learning_2010.html)

「普通とはなんぞや」という判断基準を教えこんでいく。

    機械学習
       教師あり学習
       教師なし学習
           クラスター分析
               EMアルゴリズム
               k-means法 
           主成分分析
           ベクトル量子化
           自己組織化マップ
           隠れマルコフモデル
       強化学習
           マルコフ決定過程
       識別関数
           入力データを見て、特定のクラスに属するよう識別
           サポートベクターマシン
           パーセプトロン
           ニューラルネットワーク
       識別モデル
           入力データからクラス事後確率をモデル化して識別
           CRF
       生成モデル
           入力データがどのような分布で生成されたものかをモデル化して識別
           ナイーブベイズ

#機械学習入門

*  [機械学習に基づく自然言語処理Ⅰ](http://nlp.ist.i.kyoto-u.ac.jp/member/kuro/lecture/LIP10/LIP09.pdf)(PDF) パーセプトロンやSVMを自然言語処理のどこで応用するか、など

*  [これからはじめる人のための機械学習の教科書まとめ - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20110209/1297272686)
*  [機械学習超入門 〜そろそろナイーブベイズについてひとこと言っておくか〜 - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20110114/1295030258)

    [単純]　                    [複雑]
    識別関数 < 識別モデル < 生成モデル

    識別関数  : SVM
    識別モデル: CRF
    生成モデル: ナイーブベイズ

*  [機械学習超入門II 〜Gmailの優先トレイでも使っているPA法を30分で習得しよう！〜 - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20110120/1295547335)

    スパム/非スパムではなく、メールのスコアそのものを学習する
    新しいデータ(メール)に対して、このメールはXXX点ですね、というようにスコアを教えてくれる

*  [機械学習超入門III 〜機械学習の基礎、パーセプトロンを30分で作って学ぶ〜 - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20110606/1307378609) コードあり
*  [機械学習超入門IV 〜SVM(サポートベクターマシン)だって30分で作れちゃう☆〜 - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20110627/1309188711) コードあり
*  [CRFがよくわからなくてお腹が痛くなってしまう人のための30分でわかるCRFのはなし - EchizenBlog-Zwei](http://d.hatena.ne.jp/echizen_tm/20111206/1323180144) 識別モデル


#音声認識システム

{{amazon 4627847114}}

## パターン認識システム

### 前処理部

### 特徴抽出部

### 識別部

* 最近傍決定則(NN法)
       * クラスの特徴ベクトルに最も近いプロトタイプを調べる
* k-NN法(p.51A)
       * 1. 学習データを全て読み込む
       * 2. 入力データと学習データの距離を計算
       * 3. 上記k個を取り出す
       * 4. クラスを判定する
* プロトタイプを決める
       * お手本のクラスのこと
       * 学習データから生成する
           * パーセプトロン(p.40)
           * *線形分離不可能な場合 学習が停止しない
           * SVM(p.61)
           * *線形分離可能にしてしまう
           * ニューラルネットワーク(p.71)
* 未知データ
       * ベイズの定理(p.83)
           * 事象の確率を求める

###識別辞書

#データマイニング

[332パターン認識](http://www5.ocn.ne.jp/~shinya91/csm/332csm_knn.html)

イメージが分かりやすい

*  最短距離法
*  k近傍法
*  判別分析
*  ベイズ
*  決定木
*  ニューラルネットワーク

#決定木

*  [決定木 - Wikipedia](http://ja.wikipedia.org/wiki/%E6%B1%BA%E5%AE%9A%E6%9C%A8)
*  [決定木による分類モデルとは？](http://musashi.sourceforge.jp/tutorial/mining/xtclassify/model.html)
*  あることを決めるための価値基準が固定されている
*  基準にないものが入ってくると役に立たない
*  つまり例外に対応できない
*  不確実性に対応できない
*  循環しないグラフ DAG [Directed acyclic graph - Wikipedia, the free encyclopedia](http://en.wikipedia.org/wiki/Directed_acyclic_graph)


#k近傍

*  [k近傍法 - Wikipedia](http://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95)
*  事象と解との距離
*  分類として考えることができる

探索とかマンハッタン距離とか

*  [マンハッタン距離 - Wikipedia](http://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%B3%E3%83%8F%E3%83%83%E3%82%BF%E3%83%B3%E8%B7%9D%E9%9B%A2)
*  [類似度と距離 - CatTail Wiki](http://wikiwiki.jp/cattail/?%CE%E0%BB%F7%C5%D9%A4%C8%B5%F7%CE%A5)

#k-means 法

*  [k-means法 - 機械学習の「朱鷺の杜Wiki」](http://ibisforest.org/index.php?k-means%E6%B3%95)
*  [K平均法 - Wikipedia](http://ja.wikipedia.org/wiki/K%E5%B9%B3%E5%9D%87%E6%B3%95)
*  [クラスタリングの定番アルゴリズム「K-means法」をビジュアライズしてみた - てっく煮ブログ](http://tech.nitoyon.com/ja/blog/2009/04/09/kmeans-visualise/)

#ヒューリスティック探索

[探索法](http://scl.m-kb.net/ai-3.shtml)

解までの道のりをスコアをつけて高いスコアの道を学習する


#マルコフモデル

*  マルコフ決定過程
*  現在の状態は、直前の状態が影響している
*  直前の状態のみが現在の状態に影響を与える
*  現在の状態について過去のことは無関係ではない
*  過去が連鎖して現在に至る→マルコフ連鎖


#隠れマルコフモデル

*  マルコフモデルの裏
*  連鎖が作り出されたときのその裏面を推論すること
*  目に見えるモノの裏を時系列と事象から推論する

*  [隠れマルコフモデル - Wikipedia](http://ja.wikipedia.org/wiki/%E9%9A%A0%E3%82%8C%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E3%83%A2%E3%83%87%E3%83%AB)
*  [HMM(Hidden Markov Model,隠れマルコフモデル)](http://unicorn.ike.tottori-u.ac.jp/murakami/doctor/node7.html)
*  [隠れマルコフモデルで手書き文字学習 - ゆるやかな登り坂を](http://d.hatena.ne.jp/wasya923/20110722/1311300206)
*  [隠れMarkovモデル - 機械学習の「朱鷺の杜Wiki」](http://ibisforest.org/index.php?%E9%9A%A0%E3%82%8CMarkov%E3%83%A2%E3%83%87%E3%83%AB)
*  [13.2 隠れマルコフモデル](http://www.slideshare.net/showyou/132-4775931#btnNext)


#サポートベクターマシン

*  分類などするときの A と B との距離
*  [サポートベクターマシン](http://www.sys.ci.ritsumei.ac.jp/project/theory/svm/svm.html)
*  [サポートベクターマシン（SVM）](http://www.sist.ac.jp/~kanakubo/research/neuro/supportvectormachine.html)
*  [サポート・ベクター・マシーン](http://www2.bpe.es.osaka-u.ac.jp/multineuron/multineuron_resource/svm.html)
*  [Support Vector Machine って，なに？](http://www.neuro.sfc.keio.ac.jp/~masato/study/SVM/index.htm)
*  [ところでサポートベクターマシンって何なの？ - きしだのはてな](http://d.hatena.ne.jp/nowokay/20080801/1217556122)
*  [ナイーブベイズ分類器であいさつbot作ってみた - きしだのはてな](http://d.hatena.ne.jp/nowokay/20080812#1218550621)
*  [サポートベクターマシンであいさつbotを作るためのカーネル関数 - きしだのはてな](http://d.hatena.ne.jp/nowokay/20080808#1218177501)
*  [サポートベクターマシンの本 - きしだのはてな](http://d.hatena.ne.jp/nowokay/20080715#1216142068)

#ニューラルネットワーク

*  ある事象とある事象の関係
*  事象 B が起きるためには事象 A が必要
*  0 と 1 しかない
*  発火するかしないか
*  しきい値あり

*  [ニューラルネットワーク（NN）](http://www.geocities.co.jp/SiliconValley-Cupertino/3384/nn/NN.html)
*  [ニューラルネットワーク](http://www.sist.ac.jp/~suganuma/kougi/other_lecture/SE/net/net.htm)
*  [テキストマイニングのための機械学習超入門　二夜目　パーセプトロン - あんちべ！](http://d.hatena.ne.jp/AntiBayesian/20111125/1322202138)
*  [わかりやすいパターン認識](http://nlp.dse.ibaraki.ac.jp/~shinnou/zemi1.html)
*  [ニューラルネット - 機械学習の「朱鷺の杜Wiki」](http://ibisforest.org/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88)
*  [階層型ニューラルネットワーク](http://www.geocities.jp/retort_curry119/NN.htm)
*  [バックプロパゲーションでニューラルネットの学習 - きしだのはてな](http://d.hatena.ne.jp/nowokay/20080701/1214915017)
*  [ニューラル・ネットワーク入門](http://www.ibm.com/developerworks/jp/linux/library/l-neural/)
*  [ニューラルネットワーク（NN）](http://www.geocities.co.jp/SiliconValley-Cupertino/3384/nn/NN.html)
*  [ニューラルネットワーク入門 〜目次〜](http://www-ailab.elcom.nitech.ac.jp/lecture/neuro/menu.html)
*  [PyBrainでニューラルネットワーク - パーセプトロン - F13](http://blog.f13.jp/post/21749962966/pybrain)
*  [自己組織化マップ](http://www.brain.kyutech.ac.jp/~furukawa/note/som/som.html)
*  [子供でもわかる「自己組織化マップ」](http://gaya.jp/spiking_neuron/som.htm)


#ベイジアンネットワーク

*  ある事象とある事象の関係
*  事象 B が起きるためには事象 A が必要
*  事象が起きたときの原因とその確率を推論する
*  0 か 1 かではない
*  しきい値なし
*  事象お可能性を推論する
*  不確実性に対応できる
*  [情報意味論（１１） ベイズ的プローチと事例ベースアプローチ](http://www.sakurai.comp.ae.keio.ac.jp/classes/infomean-class/lesson11/)
*  [ベイジアンネットワーク](http://www.is.doshisha.ac.jp/report/2009/01/30/20081111002/index.html)
*  [ベイジアンネットワーク：株式会社日立総合計画研究所](http://www.hitachi-hri.com/research/keyword/k52.html)
*  [ベイジアンネットワーク](http://www.sist.ac.jp/~kanakubo/research/reasoning_kr/bayesiannetwork.html)
*  [Weblog を対象とした評価表現抽出](http://www.jaist.ac.jp/ks/labs/kbs-lab/sig-swo/papers/SIG-SWO-A401/SIG-SWO-A401-02.pdf)
*  条件付き確率 P(A|B) = P(A|B) * P(B)
       *  B が起きたときの A が起きる確率
       * [条件付き確率 - Wikipedia](http://ja.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E4%BB%98%E3%81%8D%E7%A2%BA%E7%8E%87)
       * [条件付き確率](http://www.geisya.or.jp/~mwm48961/kou2/joken_p.html)

#一階述語論理

*  [一階述語論理 - Wikipedia](http://ja.wikipedia.org/wiki/%E4%B8%80%E9%9A%8E%E8%BF%B0%E8%AA%9E%E8%AB%96%E7%90%86)
*  プログラミング言語のように、文法の規則を定めるもの
*  自然言語などを処理するとき「すべての事象は規則で決定できる」というアプローチ
*  不確実性に対応できない
*  人工知能、自然言語方面では「一階述語論理」は「使えない」ものとされている

#ファジィ

*  [ファジィ集合](http://scl.m-kb.net/fuzzy-2.shtml)
*  [ファジィ演算](http://scl.m-kb.net/fuzzy-3.shtml)
*  [ファジィ推論と制御のプログラミング](http://scl.m-kb.net/fuzzy-rcpg.shtml)

#感情分析

*  [CiNii 論文 -  隠れ変数モデルによる複数語表現の感情極性分類(自然言語)](http://ci.nii.ac.jp/naid/110004852731)
*  [JAIST Repository: 構文構造を用いた感情極性分類の精度向上](https://dspace.jaist.ac.jp/dspace/handle/10119/10421)
*  [感情語辞書を用いた日本語テキストからの感情抽出](http://repository.dl.itc.u-tokyo.ac.jp/dspace/bitstream/2261/35888/1/48086413.pdf)
*  [結合価パターンへの情緒生起情報の付与](http://unicorn.ike.tottori-u.ac.jp/tokuhisa/himitsu/200403-nlp/tanaka.pdf)
*  [情緒推定の手がかりについて](http://unicorn.ike.tottori-u.ac.jp/tokuhisa/himitsu/200503-nlp/tokuhisa.pdf)
*  [対話における共感状態の分析](http://www.ipsj.or.jp/annai/aboutipsj/award/9faeag0000004ej9-att/2D_6.pdf)

#deep learning

*  [deep learningわからん & An Analysis of Single-Layer Networks in Unsupervised Feature Learning (NIPS2010)読んだメモ - 糞ネット弁慶](http://d.hatena.ne.jp/repose/20110112/1294837332)
*  [ＢＥＳＯＭ（ビーソム）ブログ 大規模ニューラルネットのブレークスルー： Deep Learning](http://besom1.blog85.fc2.com/blog-entry-97.html)
*  [ＢＥＳＯＭ（ビーソム）ブログ deep learning とは](http://besom1.blog85.fc2.com/blog-entry-104.html)[ＢＥＳＯＭ（ビーソム）ブログ deep learning とは](http://besom1.blog85.fc2.com/blog-entry-104.html)
*  [ニューラルネットの逆襲 - Preferred Research](http://research.preferred.jp/2012/11/deep-learning/)
*  [Deep learning](http://www.slideshare.net/kazoo04/deep-learning-15097274#btnNext)

#遺伝的アルゴリズム

*  [遺伝的アルゴリズムとは (イデンテキアルゴリズムとは) - ニコニコ大百科](http://dic.nicovideo.jp/a/%E9%81%BA%E4%BC%9D%E7%9A%84%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)]
*  [アルゴリズム入門 : 第 5 章 知識情報処理入門](http://msdn.microsoft.com/ja-jp/cc998607.aspx)
*  [4Gamer.net — ［CEDEC 2008＃08］生き物を相手にするようなゲームを作る〜遺伝的アルゴリズム](http://www.4gamer.net/games/051/G005101/20080911055/)
*  [遺伝的アルゴリズムとは](http://home.interlink.or.jp/~kumagai/genealg.htm)
*  [Rubyで簡単なAI（人工知能）を実装してみた（遺伝的アルゴリズム） - このブログは証明できない。](http://d.hatena.ne.jp/shunsuk/20071107/1194477381)

#[機械学習 はじめよう：連載｜gihyo.jp … 技術評論社](http://gihyo.jp/dev/serial/01/machine-learning)

##[第2回 確率の初歩](http://gihyo.jp/dev/serial/01/machine-learning/0002)

*  確率は p(X)
       *  X は「確率変数」
       *  p(X) は「 X の確率分布」あるいは単に「 X の確率」
           *  X の取り得る値 a に対してその確率を p(X=a)，または簡単に p(a) 
           *  X はサイコロをふって出る目の「確率変数」，p(X) がその「確率分布」
       *  p(X) が確率分布であるための重要な条件
           *  確率の値は0以上1以下
           *  すべての取り得る値の確率の合計は1

*  同時確率
       *  2個の確率変数 X, Y に対する確率分布を p(X, Y) と書き，XとYの「同時分布」または「同時確率」
*  条件付き確率
       *  確率変数 Y に何かある値が与えられているときのXの確率を p(X|Y) と書き，Yが与えられているときのXの「条件付き確率」
*  周辺確率
       *  確率変数 Y は気にせずに，確率変数 X のみの確率を考える場合もあります。これは p(X, Y) の X に関する「周辺確率」と言い，単純に p(X) と書きます。


ベイズの公式

http://image.gihyo.co.jp/assets/images/dev/serial/01/machine-learning/0002/thumb/TH230_EXTERN_0001.png


##[第3回 ベイジアンフィルタを実装してみよう](http://gihyo.jp/dev/serial/01/machine-learning/0003)

ベイジアンフィルタ

> ナイーブベイズ（Naive Bayes）というアルゴリズムを利用して，テキストの自動分類などに応用することのできるフィルタの総称です。事前に正しいデータを与え学習を行うので，「教師あり学習（Supervised Leaning）」と呼ばれる機械学習の手法の一つ [...] ナイーブベイズにおいてのカテゴリの推定とは，ある文章が与えられた時にその文章がどのカテゴリに属するのが「もっともらしいか」を単語の出現確率から求めます

文章（doc）が与えられた時，カテゴリ（cat）に属する確率 P(cat|doc) を求める

    P(cat, doc)
    = P(cat|doc)P(doc)
    = P(doc|cat)P(cat)

つまり

    P(cat|doc) = P(doc|cat)P(cat) / P(doc)

*  P(doc)
       *  文章が生起する確率
*  P(cat)
       *  カテゴリが生起する確率。訓練データとしてカテゴリが与えられた件数/総文書数。ナイーブベイズの訓練フェーズではカテゴリの登場回数を保存
*  P(doc|cat)
       *  あるカテゴリが与えられた時の文章の生起確率。ナイーブベイズの訓練フェーズでは出現した単語があるカテゴリに分類された回数を保存

> ナイーブベイズでは各カテゴリの登場回数と，ある文章があるカテゴリに分類された回数を保存しておけばいいのです。推定を行う時は，以上の保存された情報から P(doc|cat)P(cat) をカテゴリ毎に計算します

##[第10回ベイズ確率](http://gihyo.jp/dev/serial/01/machine-learning/0010)

*  離散確率
       *  確率の値は0以上1以下
       *  すべての取り得る値の確率の合計は1

*  連続確率
       *  確率密度関数 f(x) の値は常に0以上
       *  「取り得る値の全範囲」にわたって，関数 f(x) を積分すると1になる。つまり p(全範囲)=1 となる


*  確率の加法定理
       *  2個の確率変数 X, Y について，その同時確率 p(X,Y) と周辺確率 p(X) の間に次の等式が成り立つ。

p(Y) が離散確率の場合：

http://image.gihyo.co.jp/assets/images/dev/serial/01/machine-learning/0002/thumb/TH230_EXTERN_0000.png

p(Y) が連続確率の場合：

http://image.gihyo.co.jp/assets/images/dev/serial/01/machine-learning/0004/thumb/TH400_EXTERN_0007.png

*  確率の乗法定理
       *  2個の確率変数 X, Y について，その同時確率 p(X,Y)，条件付き確率 p(Y|X)，周辺確率 p(X) の間に次の等式が成り立つ。

    p(X, Y) = p(Y|X) p(X)


##[第15回 分類問題ことはじめ](http://gihyo.jp/dev/serial/01/machine-learning/0015)

*  分類問題
       *  データをどのようなカテゴリーに分けるかは問題を解きたい人が指定

*  クラスタリング
       *  問題を解きたい人がカテゴリを明示的に与えることはできません

人工知能まとめ

*  Why: 技術が出てきた背景
*  What: 技術の仕組み、特徴
*  How: 技術の使い方
*  Where: 技術がどこで使われているのか 
